---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "Justin Zhou"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
```

## Abstract

'''Write a brief one-paragraph abstract that describes the contents of your write-up.'''

This report revisits a public serum-proteomics dataset originally used to study potential early-detection biomarkers for autism spectrum disorder (ASD). We stress-test the analysis by varying key design choices that plausibly change which proteins look predictive. First, we justify working on the log scale and examine the influence of trimming versus not trimming outliers (Tasks 1–2). Next, we vary the feature-selection protocol by moving all selection onto a training split, expanding the number of top proteins per method, and replacing a hard intersection with a fuzzy one (Task 3). Finally, we seek either a simpler panel with comparable accuracy or an alternative panel with improved accuracy (Task 4), benchmarking against the in-class baseline. Throughout, we rely on figures and tables produced by team scripts and emphasize short, decision-oriented takeaways.

## Dataset

'''Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.'''

Source and access. Data are the public matrix from Hewitson et al. (2021, PLOS ONE; accessed for the course in Aug 2022). Following the course convention, we keep the 192 unidentified proteins that the paper dropped, to make the screening step more demanding.

Sample and variables. The analytic file contains 156 participants: ASD = 76, TD/Control = 78, with 2 unlabeled rows in the provided file. Each row is a participant; columns are serum protein intensities plus a small number of metadata fields (e.g., Group) and a clinical severity variable (ADOS Total Score). There are 1,317 protein features in this copy. Roughly 1.3% of protein cells are missing prior to preprocessing. Intensities are right-skewed and vary on a multiplicative scale, motivating a log transform; missingness is handled consistently with the team’s preprocessing script.

## Summary of published analysis

'''Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.'''

The paper builds a classifier in two stages: screening and model fitting. First, the serum protein intensities are preprocessed (log transformation; handling of extreme values) to make distributions more comparable across proteins. Next, the authors score each protein using three complementary views: (1) univariate group comparisons (ASD vs. control, one protein at a time), (2) association with clinical severity (e.g., correlation with ADOS or a similar measure), and (3) random-forest variable importance to capture multivariate, nonlinear signal. Proteins that score well across these views form a core set (via overlap across methods, either a strict intersection or “appears in at least 2 of 3”). The core is then augmented into an “optimal” panel by adding nearby candidates until classifier performance (measured on held-out data) stops improving.

With a candidate panel in hand, the study fits simple models—typically logistic regression (sometimes with forward selection) and random forest—and evaluates them on held-out data with repeated splits. Performance is summarized primarily by AUROC and overall accuracy, reported alongside the final list of proteins in the panel. In other words, the screening stage decides what to measure, and the classifier stage tests how well those choices predict in data that were not used for selection.

Key results to report in this write-up: The core set comprised [ ], which after augmentation yielded the final (“optimal”) panel of [ ]. On held-out evaluation, the paper reports AUROC ≈ [__] and accuracy ≈ [__]. 

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

Tasks 1-2

### Methodological variations

Task 3

### Improved classifier

Task 4
