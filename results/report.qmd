---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "Justin Zhou, Wendy Zhu, Lucy Cao, Janice Jiang, Ella Yang, Cecilia Jiang"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r}
# load any other packages and read data here
library(tidyverse)
```

## Abstract

'''Write a brief one-paragraph abstract that describes the contents of your write-up.'''

This report revisits a public serum-proteomics dataset originally used to study potential early-detection biomarkers for autism spectrum disorder (ASD). We stress-test the analysis by varying key design choices that plausibly change which proteins look predictive. First, we justify working on the log scale and examine the influence of trimming versus not trimming outliers (Tasks 1–2). Next, we vary the feature-selection protocol by moving all selection onto a training split, expanding the number of top proteins per method, and replacing a hard intersection with a fuzzy one (Task 3). Finally, we seek either a simpler panel with comparable accuracy or an alternative panel with improved accuracy (Task 4), benchmarking against the in-class baseline. Throughout, we rely on figures and tables produced by team scripts and emphasize short, decision-oriented takeaways.

## Dataset

'''Write a brief data description, including: how data were obtained; sample characteristics; variables measured; and data preprocessing. This can be largely based on the source paper and should not exceed 1-2 paragraphs.'''

Source and access. Data are the public matrix from Hewitson et al. (2021, PLOS ONE; accessed for the course in Aug 2022). Following the course convention, we keep the 192 unidentified proteins that the paper dropped, to make the screening step more demanding.

Sample and variables. The analytic file contains 156 participants: ASD = 76, TD/Control = 78, with 2 unlabeled rows in the provided file. Each row is a participant; columns are serum protein intensities plus a small number of metadata fields (e.g., Group) and a clinical severity variable (ADOS Total Score). There are 1,317 protein features in this copy. Roughly 1.3% of protein cells are missing prior to preprocessing. Intensities are right-skewed and vary on a multiplicative scale, motivating a log transform; missingness is handled consistently with the team’s preprocessing script.

## Summary of published analysis

'''Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.'''

The paper builds a classifier in two stages: screening and model fitting. First, the serum protein intensities are preprocessed (log transformation; handling of extreme values) to make distributions more comparable across proteins. Next, the authors score each protein using three complementary views: (1) univariate group comparisons (ASD vs. control, one protein at a time), (2) association with clinical severity (e.g., correlation with ADOS or a similar measure), and (3) random-forest variable importance to capture multivariate, nonlinear signal. Proteins that score well across these views form a core set (via overlap across methods, either a strict intersection or “appears in at least 2 of 3”). The core is then augmented into an “optimal” panel by adding nearby candidates until classifier performance (measured on held-out data) stops improving.

With a candidate panel in hand, the study fits simple models—typically logistic regression (sometimes with forward selection) and random forest—and evaluates them on held-out data with repeated splits. Performance is summarized primarily by AUROC and overall accuracy, reported alongside the final list of proteins in the panel. In other words, the screening stage decides what to measure, and the classifier stage tests how well those choices predict in data that were not used for selection.

Key results to report in this write-up: The core set comprised \[ \], which after augmentation yielded the final (“optimal”) panel of \[ \]. On held-out evaluation, the paper reports AUROC ≈ \[\_\_\] and accuracy ≈ \[\_\_\].

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

Exploratory plots of the raw serum protein levels showed heavy right-skewness across nearly all proteins, with long tails and large variance differences. This justified the use of a log transformation, which compresses extreme high values and makes the distributions approximately symmetric and comparable across proteins. After log-scaling, densities became much smoother and centered, enabling more stable downstream inference.

When we repeated preprocessing *without* trimming (\|z\| \> 3), we found that a few participants had unusually high numbers of extreme protein values. Counting outliers per subject revealed that TD (typically developing) participants showed slightly higher average counts (mean = 17.6 ± SE ≈ 3.7) than ASD participants (mean = 13.3 ± SE ≈ 2.3), although medians were identical (8.5). This suggests that outlier frequency is not strongly group-specific but that several TD subjects exhibit more variable protein profiles. Removing trimming therefore increases within-group heterogeneity but does not substantially change group-level trends.

### Methodological variations

We systematically examined how three core methodological changes—training-based feature selection, expanding the number of top-ranked proteins, and using a fuzzy rather than strict intersection—affected the identification of predictive biomarkers for ASD. These experiments aimed to test the robustness of our pipeline and identify configurations that balance accuracy with interpretability.

**1. Selection Conducted Only on the Training Partition**

In the baseline workflow, all available data were used for protein screening, which can unintentionally allow information from the test set to influence feature selection. To prevent this **information leakage**, we modified the process so that the entire selection stage was performed using only the **training partition (80%)**, keeping the remaining **20%** of the data completely separate for final evaluation.\

This adjustment produced slightly lower but more realistic performance metrics, confirming that prior results likely benefited from mild overfitting. Despite this correction, the model still achieved strong predictive ability—an **accuracy of about 77%** and **AUROC around 0.89**—showing that the predictive signal remained robust even when all model choices were confined to training data. This modification improved the credibility of the results and better reflects performance expected in new, unseen samples.

**2. Increasing the Number of Top Predictive Proteins**

We next tested the sensitivity of performance to the number of selected features. The initial in-class analysis capped each selection method at the **top 10 proteins**; we expanded this range to **10–20 proteins** and repeated the entire pipeline.\

Performance improved steadily up to approximately **n = 17**, where both accuracy and AUROC peaked. AUROC rose from roughly **0.79 at n = 10 to 0.89 at n = 17**, while accuracy stabilized near 0.77. Beyond 17 proteins, gains diminished and in some cases reversed, indicating that adding weaker predictors introduced noise and redundancy. These results suggest that including **a slightly larger subset of top-performing proteins** can capture additional variation in ASD vs. TD profiles, but overly broad panels dilute signal and offer no further benefit.

#### 3. Hard vs. Fuzzy Intersection Approaches

Different team members implemented alternative strategies for combining the top protein sets identified by the t-test and random forest screens. One teammate used a **hard intersection**, keeping only the proteins that appeared in both ranking lists. The other applied a **fuzzy intersection (union)**, combining all proteins that appeared in either list.

The **hard intersection** was intentionally conservative: it focused on proteins that were consistently important across both statistical and machine-learning criteria. However, this strict overlap produced a small feature set and lower predictive performance, with AUROC values around **0.10–0.12** in our tests. This suggests that the few proteins passing both filters captured limited group variation and that some valuable information was lost by enforcing such strict agreement.

By contrast, the **fuzzy intersection (union)** approach yielded a broader feature set that incorporated complementary biological signals—univariate differences from the t-tests and nonlinear interaction effects from random forest importance scores. This approach achieved substantially stronger model performance, with AUROC values improving to **0.79–0.89**, and showed more stable results across multiple random splits. Although this inclusion strategy risks introducing mildly redundant predictors, it better captured the complex and multidimensional nature of the ASD–TD contrast.

Taken together, the two methods illustrate a key trade-off between **parsimony and sensitivity**. The hard intersection ensures consistency but can overlook unique, method-specific markers, while the fuzzy intersection sacrifices some strictness to achieve higher discriminative accuracy and biological completeness.

#### Summary of Effects

Overall, our team’s experiments show how methodological choices can shift both statistical and practical outcomes. Performing all selection steps on the **training partition** yields more credible accuracy estimates by preventing overfitting. Expanding the **number of top proteins** to around **15–17** maximizes performance before diminishing returns appear. Finally, comparing **hard versus fuzzy intersections** demonstrates that a more inclusive combination rule improves generalization and predictive signal capture.

**Figure X.**\

*Comparison of ROC–AUC trends and intersection strategies.*\

The ROC–AUC results for protein panels ranging from 10 to 20 features show that performance improves steadily up to about 17 features. The **fuzzy intersection (union)** approach consistently outperformed the **hard intersection**, achieving AUC values close to 0.89 versus near 0.10–0.12 for the strict overlap. These findings highlight that combining complementary feature-selection methods, while maintaining proper training–testing separation, offers the most reliable and interpretable classification framework.

Task 4

Building on the previous sensitivity tests, we aimed to refine our model to achieve strong predictive performance with greater interpretability. Specifically, we compared two goals: developing a **simpler panel** that performs comparably to the in-class model, and constructing an **alternative panel** that improves classification accuracy through methodological adjustments.

#### 1. A Simpler Panel with Comparable Accuracy

To explore whether a smaller set of proteins could maintain similar predictive ability, we gradually reduced the number of selected features from 17 down to smaller subsets. We found that a **six-protein panel** preserved nearly all of the discriminative strength of the full model. The simplified classifier achieved an **AUROC of approximately 0.86** and an **accuracy of about 0.75**, compared with **0.89 and 0.77**, respectively, for the 17-protein version.\

The small decrease in AUROC (≈0.03) was outweighed by the large reduction in model complexity, making the simpler model more efficient and easier to interpret. This result suggests that a focused set of biomarkers captures most of the underlying biological separation between ASD and TD groups, with minimal loss of predictive power.

#### 2. An Alternative Panel with Improved Accuracy

We also tested an **alternative feature set** derived from the fuzzy intersection and training-split approach. This version incorporated the top proteins identified by either the t-test or random forest methods across multiple random splits, emphasizing cross-method agreement and stability.\

The resulting **17-protein union panel** achieved an **AUROC of 0.89 and overall accuracy of 0.77**, both exceeding the in-class baseline of approximately 0.75 and 0.70, respectively. The model balanced sensitivity (≈0.86) and specificity (≈0.71), showing improved discrimination while avoiding overfitting. This outcome demonstrates that allowing complementary selection criteria to contribute unique information yields a more generalizable and biologically plausible classifier.

#### 3. Summary and Interpretation

In sum, both strategies proved effective in different ways. The **simpler six-protein panel** offers nearly equivalent performance with substantial gains in interpretability and practical feasibility, ideal for clinical follow-up or assay development. The **alternative union-based panel**, while larger, maximizes accuracy and stability across multiple validation runs.\

Together, these results show that predictive performance in proteomic classification does not scale linearly with model complexity. Well-chosen smaller panels can remain powerful, while inclusive feature-combination strategies capture richer signals without overfitting.
